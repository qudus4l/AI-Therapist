<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Mindful AI Therapy</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@300;400;500&display=swap" rel="stylesheet">
  <style>
    :root {
      --primary-bg: #f5f7fa;
      --secondary-bg: rgba(255, 255, 255, 0.95);
      --accent-color: #8ba5c9;
      --accent-glow: rgba(139, 165, 201, 0.2);
      --text-primary: #2d3c4e;
      --text-secondary: #64748b;
      --success-color: #9cbeb5;
      --error-color: #e17c7c;
      --shadow-sm: 0 2px 4px rgba(0,0,0,0.03);
      --shadow-md: 0 4px 6px rgba(0,0,0,0.05);
      --shadow-lg: 0 8px 24px rgba(0,0,0,0.05);
      --transition-default: all 0.4s cubic-bezier(0.4, 0, 0.2, 1);
      --gradient-primary: linear-gradient(135deg, #f5f7fa 0%, #e8eef5 100%);
      --gradient-accent: linear-gradient(135deg, #8ba5c9 0%, #a3b8d9 100%);
      --gradient-ambient: linear-gradient(135deg, rgba(139, 165, 201, 0.1) 0%, rgba(163, 184, 217, 0.1) 100%);
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Plus Jakarta Sans', sans-serif;
      line-height: 1.6;
      background: var(--gradient-primary);
      color: var(--text-primary);
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      position: relative;
      overflow-x: hidden;
    }

    /* Ambient background animation */
    body::before {
      content: '';
      position: fixed;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: var(--gradient-ambient);
      z-index: -1;
      animation: ambientShift 15s ease-in-out infinite alternate;
    }

    @keyframes ambientShift {
      0% { opacity: 0.5; transform: scale(1); }
      100% { opacity: 0.8; transform: scale(1.1); }
    }
    
    .container {
      max-width: 1200px;
      margin: 0 auto;
      padding: 2rem;
      flex: 1;
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 2.5rem;
      position: relative;
      z-index: 1;
    }

    h1 {
      color: var(--text-primary);
      font-weight: 300;
      font-size: 2.75rem;
      text-align: center;
      margin: 2rem 0;
      letter-spacing: -0.5px;
      opacity: 0;
      animation: fadeIn 1s ease-out forwards;
    }

    @keyframes fadeIn {
      to { opacity: 1; transform: translateY(0); }
    }

    .controls {
      display: flex;
      gap: 1.2rem;
      margin-bottom: 1rem;
      opacity: 0;
      animation: fadeIn 1s ease-out 0.3s forwards;
    }

    button {
      background: var(--gradient-accent);
      color: white;
      border: none;
      padding: 1rem 2rem;
      border-radius: 16px;
      cursor: pointer;
      font-size: 1.1rem;
      font-weight: 500;
      transition: var(--transition-default);
      box-shadow: var(--shadow-md), 0 0 0 var(--accent-glow);
      position: relative;
      overflow: hidden;
    }

    button::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: var(--gradient-accent);
      opacity: 0;
      transition: var(--transition-default);
    }

    button:hover {
      transform: translateY(-2px);
      box-shadow: var(--shadow-lg), 0 0 20px var(--accent-glow);
    }

    button:hover::before {
      opacity: 0.2;
    }

    #crisis-alert {
      display: none;
      background: var(--error-color);
      color: white;
      padding: 1.2rem;
      border-radius: 16px;
      position: fixed;
      top: 2rem;
      left: 50%;
      transform: translateX(-50%);
      z-index: 1000;
      box-shadow: var(--shadow-lg);
      animation: slideDown 0.5s cubic-bezier(0.4, 0, 0.2, 1);
      backdrop-filter: blur(10px);
    }

    @keyframes slideDown {
      from { transform: translate(-50%, -100%); opacity: 0; }
      to { transform: translate(-50%, 0); opacity: 1; }
    }

    .voice-visualizer {
      width: 220px;
      height: 220px;
      position: relative;
      margin: 2rem auto;
      opacity: 0;
      animation: fadeIn 1s ease-out 0.6s forwards;
    }

    .voice-circle {
      position: relative;
      width: 100%;
      height: 100%;
    }

    .circle-base {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border-radius: 50%;
      background: radial-gradient(circle at center,
        rgba(139, 165, 201, 0.2),
        rgba(163, 184, 217, 0.15),
        rgba(139, 165, 201, 0.1),
        rgba(163, 184, 217, 0.05)
      );
      box-shadow: 
        0 0 40px var(--accent-glow),
        inset 0 0 20px rgba(255, 255, 255, 0.5);
      transition: var(--transition-default);
    }

    .circle-pulse {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      border-radius: 50%;
      background: radial-gradient(circle at center,
        rgba(139, 165, 201, 0.3),
        rgba(163, 184, 217, 0.2),
        rgba(139, 165, 201, 0.1),
        rgba(163, 184, 217, 0.05)
      );
      animation: gentlePulse 4s ease-in-out infinite;
      opacity: 0;
      box-shadow: 0 0 30px var(--accent-glow);
    }

    .speaking .circle-base {
      transform: scale(1.05);
      box-shadow: 
        0 0 60px var(--accent-glow),
        inset 0 0 30px rgba(255, 255, 255, 0.6);
    }

    .speaking .circle-pulse {
      opacity: 0.3;
    }

    @keyframes gentlePulse {
      0% { transform: scale(0.95); opacity: 0.2; }
      50% { transform: scale(1.05); opacity: 0.3; }
      100% { transform: scale(0.95); opacity: 0.2; }
    }

    .voice-label {
      position: absolute;
      bottom: -2.5rem;
      left: 50%;
      transform: translateX(-50%);
      font-size: 1rem;
      color: var(--text-secondary);
      font-weight: 400;
      white-space: nowrap;
      text-align: center;
    }

    .conversation {
      width: 100%;
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem;
      background: var(--secondary-bg);
      border-radius: 24px;
      box-shadow: var(--shadow-lg);
      height: 400px;
      overflow-y: auto;
      scrollbar-width: thin;
      scrollbar-color: var(--accent-color) var(--primary-bg);
      backdrop-filter: blur(10px);
      opacity: 0;
      animation: fadeIn 1s ease-out 0.9s forwards;
    }

    .conversation::-webkit-scrollbar {
      width: 6px;
    }

    .conversation::-webkit-scrollbar-track {
      background: var(--primary-bg);
      border-radius: 3px;
    }

    .conversation::-webkit-scrollbar-thumb {
      background-color: var(--accent-color);
      border-radius: 3px;
    }

    .message {
      margin: 1rem 0;
      padding: 1.2rem 1.8rem;
      border-radius: 16px;
      max-width: 85%;
      line-height: 1.6;
      font-size: 1rem;
      transition: var(--transition-default);
      opacity: 0;
      transform: translateY(10px);
      animation: messageAppear 0.5s ease-out forwards;
    }

    @keyframes messageAppear {
      to { opacity: 1; transform: translateY(0); }
    }

    .message.user {
      background: var(--gradient-ambient);
      margin-left: auto;
      color: var(--text-primary);
      box-shadow: var(--shadow-sm);
    }

    .message.therapist {
      background: rgba(139, 165, 201, 0.1);
      margin-right: auto;
      color: var(--text-primary);
      box-shadow: var(--shadow-sm);
    }

    .message.system {
      background: rgba(255, 255, 255, 0.5);
      margin: 1rem auto;
      font-size: 0.9rem;
      color: var(--text-secondary);
      text-align: center;
      backdrop-filter: blur(5px);
    }

    .status-indicator {
      position: fixed;
      bottom: 0;
      left: 0;
      right: 0;
      padding: 1rem;
      background: var(--secondary-bg);
      box-shadow: var(--shadow-lg);
      z-index: 100;
      font-size: 0.9rem;
      display: flex;
      justify-content: center;
      gap: 2.5rem;
      color: var(--text-secondary);
      backdrop-filter: blur(10px);
      opacity: 0;
      animation: fadeIn 1s ease-out 1.2s forwards;
    }

    .status-indicator p {
      display: flex;
      align-items: center;
      gap: 0.5rem;
    }

    .status-indicator span {
      font-weight: 500;
      color: var(--text-primary);
    }

    @media (max-width: 768px) {
      .container {
        padding: 1rem;
      }

      h1 {
        font-size: 2rem;
      }

      .conversation {
        height: 350px;
        margin-bottom: 4rem;
        padding: 1.5rem;
      }

      .status-indicator {
        flex-direction: column;
        align-items: center;
        gap: 0.5rem;
        padding: 0.8rem;
      }

      .voice-visualizer {
        width: 180px;
        height: 180px;
      }

      button {
        padding: 0.8rem 1.6rem;
        font-size: 1rem;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Welcome to Your Safe Space</h1>
    
    <div class="controls">
      <button id="start-button">Begin Your Journey</button>
      <button id="end-button" style="display: none;">End Session</button>
    </div>

    <div id="crisis-alert"></div>
    
    <div class="voice-visualizer">
      <div class="voice-circle">
        <div class="circle-base"></div>
        <div class="circle-pulse"></div>
      </div>
      <div class="voice-label">I'm here to listen and support you...</div>
    </div>

    <div class="conversation" id="conversation">
      <!-- Messages will be added here dynamically -->
    </div>

    <div class="status-indicator">
      <p>Connection: <span id="status">Ready to begin when you are</span></p>
      <p>Microphone: <span id="mic-status">Waiting for permission</span></p>
      <p>Voice: <span id="audio-level">-</span></p>
    </div>
  </div>

  <script>
    const statusEl = document.getElementById('status');
    const micStatusEl = document.getElementById('mic-status');
    const audioLevelEl = document.getElementById('audio-level');
    const startBtn = document.getElementById('start-button');
    const endBtn = document.getElementById('end-button');
    const crisisAlertEl = document.getElementById('crisis-alert');
    const conversationEl = document.getElementById('conversation');
    let currentSessionId = null;  // Add this to track the session ID
    let peerConnection = null;  // Add this to track the WebRTC connection
    let audioStream = null;     // Add this to track the audio stream
    let dataChannel = null;     // Add this to track the data channel

    // Add new variables for voice visualization
    const voiceCircle = document.querySelector('.voice-circle');
    const voiceLabel = document.querySelector('.voice-label');
    let isTherapistSpeaking = false;
    let userSpeakingTimeout;

    // Update audio level monitoring to control visualization
    function setupAudioLevelMonitoring(stream) {
      const audioContext = new AudioContext();
      const microphone = audioContext.createMediaStreamSource(stream);
      const analyser = audioContext.createAnalyser();
      analyser.fftSize = 256;
      microphone.connect(analyser);
      
      const dataArray = new Uint8Array(analyser.frequencyBinCount);
      
      function checkAudioLevel() {
        analyser.getByteFrequencyData(dataArray);
        const average = dataArray.reduce((a, b) => a + b) / dataArray.length;
        audioLevelEl.textContent = Math.round(average);
        
        // Update voice visualization
        if (average > 5 && !isTherapistSpeaking) {
          voiceCircle.classList.add('speaking');
          voiceLabel.textContent = 'Speaking...';
          clearTimeout(userSpeakingTimeout);
          userSpeakingTimeout = setTimeout(() => {
            if (!isTherapistSpeaking) {
              voiceCircle.classList.remove('speaking');
              voiceLabel.textContent = 'Listening...';
            }
          }, 300);
        }

        if (average < 5) {
          micStatusEl.style.color = 'red';
          micStatusEl.textContent = 'No audio detected - check your microphone';
        } else {
          micStatusEl.style.color = 'green';
          micStatusEl.textContent = 'Audio detected';
        }
        requestAnimationFrame(checkAudioLevel);
      }
      
      checkAudioLevel();
    }

    // Update message handling to show therapist speaking state
    function addMessage(text, role) {
      const messageEl = document.createElement('div');
      messageEl.className = `message ${role}`;
      messageEl.textContent = text;
      conversationEl.appendChild(messageEl);
      conversationEl.scrollTop = conversationEl.scrollHeight;

      if (role === 'therapist') {
        isTherapistSpeaking = true;
        voiceCircle.classList.add('speaking');
        voiceLabel.textContent = 'Therapist speaking...';
        // Simulate end of therapist speaking after message display
        setTimeout(() => {
          isTherapistSpeaking = false;
          voiceCircle.classList.remove('speaking');
          voiceLabel.textContent = 'Listening...';
        }, 2000);
      }
    }

    function handleCrisisDetection(crisisData) {
      if (crisisData.crisis_level !== 'none') {
        crisisAlertEl.style.display = 'block';
        crisisAlertEl.textContent = `Crisis Alert: ${crisisData.reason}`;
        if (crisisData.crisis_level === 'emergency') {
          addMessage("I hear that you're in crisis. Please know that I'm an AI and cannot provide emergency services. Please contact emergency services or crisis hotline immediately at 988.", 'therapist');
        }
      } else {
        crisisAlertEl.style.display = 'none';
      }
    }

    async function initRealtimeConnection() {
      try {
        startBtn.style.display = 'none';
        endBtn.style.display = 'inline-block';
        statusEl.textContent = "Fetching ephemeral key...";
        const tokenResponse = await fetch("/session");
        if (!tokenResponse.ok) {
          throw new Error("Failed to get ephemeral key from /session");
        }
        const tokenData = await tokenResponse.json();
        currentSessionId = tokenData.session_id;  // Store the session ID
        const EPHEMERAL_KEY = tokenData.client_secret.value;

        statusEl.textContent = "Setting up WebRTC PeerConnection...";
        peerConnection = new RTCPeerConnection({
          iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
        });

        // Setup audio element for model's response
        const audioEl = document.createElement("audio");
        audioEl.autoplay = true;
        document.body.appendChild(audioEl);

        peerConnection.ontrack = (e) => {
          console.log("Received remote track:", e);
          audioEl.srcObject = e.streams[0];
        };

        // Add local microphone with error handling
        try {
          micStatusEl.textContent = "Requesting microphone access...";
          audioStream = await navigator.mediaDevices.getUserMedia({ 
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true
            } 
          });
          
          micStatusEl.textContent = "Microphone connected";
          setupAudioLevelMonitoring(audioStream);
          
          audioStream.getTracks().forEach(track => {
            console.log("Adding local track:", track);
            peerConnection.addTrack(track, audioStream);
          });
        } catch (micError) {
          console.error("Microphone error:", micError);
          micStatusEl.textContent = `Microphone error: ${micError.message}`;
          throw new Error(`Failed to access microphone: ${micError.message}`);
        }

        // Data channel setup with error handling
        dataChannel = peerConnection.createDataChannel("oai-events");
        dataChannel.onopen = () => {
          console.log("Data channel is open");
          statusEl.textContent = "Data channel opened";
        };
        dataChannel.onerror = (error) => {
          console.error("Data channel error:", error);
          statusEl.textContent = `Data channel error: ${error.message}`;
        };
        dataChannel.addEventListener("message", (e) => {
          try {
            const realtimeEvent = JSON.parse(e.data);
            console.log("Received event:", realtimeEvent);
            
            // Handle different event types
            if (realtimeEvent.type === 'response.done') {
              const output = realtimeEvent.response.output[0];
              if (output.type === 'function_call' && output.name === 'detect_crisis') {
                handleCrisisDetection(JSON.parse(output.arguments));
              } else if (output.type === 'text') {
                addMessage(output.text, 'therapist');
              }
            }
          } catch (err) {
            console.warn("Received non-JSON data:", e.data);
          }
        });

        // ICE connection monitoring
        peerConnection.oniceconnectionstatechange = () => {
          console.log("ICE connection state:", peerConnection.iceConnectionState);
          statusEl.textContent = `ICE connection: ${peerConnection.iceConnectionState}`;
        };

        // Create and set local description
        const offer = await peerConnection.createOffer();
        await peerConnection.setLocalDescription(offer);

        statusEl.textContent = "Sending SDP offer to Realtime API...";
        const baseUrl = "https://api.openai.com/v1/realtime";
        const model = "gpt-4o-realtime-preview-2024-12-17";

        const sdpResponse = await fetch(`${baseUrl}?model=${model}`, {
          method: "POST",
          body: offer.sdp,
          headers: {
            "Authorization": `Bearer ${EPHEMERAL_KEY}`,
            "Content-Type": "application/sdp"
          }
        });

        if (!sdpResponse.ok) {
          throw new Error(`Realtime API SDP exchange failed: ${await sdpResponse.text()}`);
        }

        const answer = {
          type: "answer",
          sdp: await sdpResponse.text(),
        };
        await peerConnection.setRemoteDescription(answer);

        statusEl.textContent = "WebRTC connection established!";

        // Send initial prompt
        setTimeout(() => {
          const initEvent = {
            type: "response.create",
            response: {
              modalities: ["text", "speech"],
              instructions: "Welcome to your therapy session. I'm here to listen and support you. How are you feeling today?",
            },
          };
          dataChannel.send(JSON.stringify(initEvent));
          addMessage("Welcome to your therapy session. I'm here to listen and support you. How are you feeling today?", 'therapist');
        }, 1000);

      } catch (err) {
        console.error(err);
        statusEl.textContent = "Error: " + err.message;
        micStatusEl.textContent = "Connection failed";
      }
    }

    endBtn.addEventListener('click', async () => {
      try {
        if (currentSessionId) {
          statusEl.textContent = 'Ending session...';
          
          // Close data channel first if it exists and is open
          if (dataChannel && dataChannel.readyState === "open") {
            console.log("Closing data channel");
            dataChannel.close();
            dataChannel = null;
          }

          // Close WebRTC connection
          if (peerConnection) {
            console.log("Closing peer connection");
            peerConnection.close();
            peerConnection = null;
          }
          
          // Stop all audio tracks
          if (audioStream) {
            console.log("Stopping audio tracks");
            audioStream.getTracks().forEach(track => {
              track.stop();
              console.log("Stopped track:", track.kind);
            });
            audioStream = null;
          }

          const response = await fetch(`/session/${currentSessionId}/end`, {
            method: 'POST'
          });
          
          if (!response.ok) {
            throw new Error('Failed to end session');
          }

          const data = await response.json();
          console.log('Session ended:', data);
          
          // Display final message
          addMessage("Thank you for sharing with me today. Take care of yourself.", 'therapist');
          
          // Reset UI
          startBtn.style.display = 'inline-block';
          endBtn.style.display = 'none';
          statusEl.textContent = 'Session ended';
          currentSessionId = null;
          
          // Optional: Display session summary
          if (data.summary) {
            const duration = new Date(data.summary.session_end) - new Date(data.summary.session_start);
            const minutes = Math.round(duration / 1000 / 60);
            addMessage(`Session duration: ${minutes} minutes. Crisis events: ${data.summary.crisis_events.length}`, 'system');
          }
        }
      } catch (error) {
        console.error('Error ending session:', error);
        statusEl.textContent = 'Error ending session';
      }
    });

    startBtn.addEventListener("click", initRealtimeConnection);
  </script>
</body>
</html>
